{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Assignment 3: Pandas**\n",
    "- **you will learn:** how to create and manipulate Pandas series and dataframes, perform vectorized computations and work with dates and times\n",
    "- **task:**  See section 3.4 below\n",
    "- **deadline:** 03.11.2025\n",
    "- [Pandas documentation](https://pandas.pydata.org/docs/index.html)\n",
    "- üìù **Reminder:** Sync your GitHub repository with the main course repository, update your project in PyCharm, and after completing the assignment, commit and push your changes back to GitHub."
   ],
   "id": "2f18feb73529a772"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **3.1 Introduction: Connection to NumPy**\n",
    "\n",
    "The **Pandas** library is a high-level data manipulation tool built on top of **NumPy**.  \n",
    "While NumPy provides powerful *numerical* operations on arrays, Pandas adds tools for handling **labeled**, **heterogeneous**, and **tabular data** ‚Äî similar to how you might work with data in an Excel sheet or SQL table.\n",
    "\n",
    "- NumPy ‚Üí works with arrays of numbers  \n",
    "- Pandas ‚Üí works with **Series** (1D labeled data) and **DataFrames** (2D labeled data)\n",
    "\n",
    "You can think of **Pandas** as an enhanced version of NumPy ‚Äî it builds upon its speed and efficiency, but adds **labels**, **indexing**, and **data handling** capabilities that make real-world data analysis much easier."
   ],
   "id": "bdb1a091098bc1ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T10:43:59.124887Z",
     "start_time": "2025-11-01T10:43:59.110550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)"
   ],
   "id": "3ff8565013b4e292",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.3.3\n",
      "Pandas version: 2.3.2\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **3.2 Series**\n",
    "\n",
    "There are two main data structures in Pandas:\n",
    "\n",
    "1. **Series** ‚Äì a one-dimensional labeled array (like a single column)\n",
    "2. **DataFrame** ‚Äì a two-dimensional labeled data structure (like a table)\n",
    "\n",
    "We begin by inspecting the **Series** data structure: a sequence of values with an associated index (which makes it different from a NumPy array)."
   ],
   "id": "7e36b042a6842a2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:39:31.883733Z",
     "start_time": "2025-10-27T11:39:31.843614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Series comes with data, index and possibly a name. \\n\")\n",
    "\n",
    "# 1Ô∏è‚É£ Floats with default index\n",
    "s1 = pd.Series([1.2, 3.4, 5.6, 7.8])\n",
    "print(\"1. Floats with default index:\\n\", s1, \"\\n\")\n",
    "\n",
    "# 2Ô∏è‚É£ Floats with explicit string index\n",
    "s2 = pd.Series([2.5, 3.8, 4.1], index=['a', 'b', 'c'])\n",
    "print(\"2. Floats with string index:\\n\", s2, \"\\n\")\n",
    "\n",
    "# 3Ô∏è‚É£ Floats with consecutive date index\n",
    "s3 = pd.Series([10.1, 11.2, 12.3, 13.4], index=pd.date_range(\"2025-01-01\", periods=4))\n",
    "print(\"3. Floats with date index:\\n\", s3, \"\\n\")\n",
    "\n",
    "# 4Ô∏è‚É£ Mixed data types (int, float, string, bool)\n",
    "s4 = pd.Series([10, 3.14, 'hello', True], name='A bunch of data...')\n",
    "print(\"4. Mixed data types:\\n\", s4, \"\\n\")\n",
    "\n",
    "# 5Ô∏è‚É£ Data containing missing values (NaN)\n",
    "s5 = pd.Series([1.1, np.nan, 2.2, None, 3.3])\n",
    "print(\"5. Data with missing values:\\n\", s5)\n",
    "\n",
    "# 6Ô∏è‚É£ Data created from NumPy array\n",
    "arr = np.array([100, 200, 300])\n",
    "s6 = pd.Series(arr)\n",
    "print(\"\\n6. Data from NumPy array:\\n\", s6)"
   ],
   "id": "6a1ebecc8ea08797",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series comes with data, index and possibly a name. \n",
      "\n",
      "1. Floats with default index:\n",
      " 0    1.2\n",
      "1    3.4\n",
      "2    5.6\n",
      "3    7.8\n",
      "dtype: float64 \n",
      "\n",
      "2. Floats with string index:\n",
      " a    2.5\n",
      "b    3.8\n",
      "c    4.1\n",
      "dtype: float64 \n",
      "\n",
      "3. Floats with date index:\n",
      " 2025-01-01    10.1\n",
      "2025-01-02    11.2\n",
      "2025-01-03    12.3\n",
      "2025-01-04    13.4\n",
      "Freq: D, dtype: float64 \n",
      "\n",
      "4. Mixed data types:\n",
      " 0       10\n",
      "1     3.14\n",
      "2    hello\n",
      "3     True\n",
      "Name: A bunch of data..., dtype: object \n",
      "\n",
      "5. Data with missing values:\n",
      " 0    1.1\n",
      "1    NaN\n",
      "2    2.2\n",
      "3    NaN\n",
      "4    3.3\n",
      "dtype: float64\n",
      "\n",
      "6. Data from NumPy array:\n",
      " 0    100\n",
      "1    200\n",
      "2    300\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Also the index of a Series has a specific data type, which can vary based on how the Series was created.",
   "id": "4887415dae5a8e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:39:31.945892Z",
     "start_time": "2025-10-27T11:39:31.928837Z"
    }
   },
   "cell_type": "code",
   "source": "s1.index, s2.index, s3.index, s4.index, s5.index, s6.index",
   "id": "7e175191cf08b769",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RangeIndex(start=0, stop=4, step=1),\n",
       " Index(['a', 'b', 'c'], dtype='object'),\n",
       " DatetimeIndex(['2025-01-01', '2025-01-02', '2025-01-03', '2025-01-04'], dtype='datetime64[ns]', freq='D'),\n",
       " RangeIndex(start=0, stop=4, step=1),\n",
       " RangeIndex(start=0, stop=5, step=1),\n",
       " RangeIndex(start=0, stop=3, step=1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:39:32.081750Z",
     "start_time": "2025-10-27T11:39:32.064284Z"
    }
   },
   "cell_type": "code",
   "source": "s1[2], s2['b'], s3[pd.Timestamp('2025-01-02')], s4[3], s5[1], s6[len(s6)-1]",
   "id": "bbaeaf690be932dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(5.6),\n",
       " np.float64(3.8),\n",
       " np.float64(11.2),\n",
       " True,\n",
       " np.float64(nan),\n",
       " np.int64(300))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:39:32.196899Z",
     "start_time": "2025-10-27T11:39:32.182816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Even with non-integer indices, we can access elements using .iloc:\\n\")\n",
    "s3.iloc[2]"
   ],
   "id": "3a7f591105c5f7ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even with non-integer indices, we can access elements using .iloc:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(12.3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:39:32.391498Z",
     "start_time": "2025-10-27T11:39:32.373120Z"
    }
   },
   "cell_type": "code",
   "source": "s2[['a', 'c']]",
   "id": "20d73c86ba0af13b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    2.5\n",
       "c    4.1\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:39:32.542282Z",
     "start_time": "2025-10-27T11:39:32.531130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Slicing works similarly to NumPy arrays:\\n\")\n",
    "s1[1:-1]"
   ],
   "id": "a8a051c5b5232089",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing works similarly to NumPy arrays:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    3.4\n",
       "2    5.6\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:40:00.428209Z",
     "start_time": "2025-10-27T11:40:00.411675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"For non-integer indices, slicing works even with interval boundaries:\\n\")\n",
    "s3['2025-01-02':'2025-01-04'], s3[1:3]"
   ],
   "id": "9043ee369c4ed3ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For non-integer indices, slicing works even with interval boundaries:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2025-01-02    11.2\n",
       " 2025-01-03    12.3\n",
       " 2025-01-04    13.4\n",
       " Freq: D, dtype: float64,\n",
       " 2025-01-02    11.2\n",
       " 2025-01-03    12.3\n",
       " Freq: D, dtype: float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T12:28:51.552188Z",
     "start_time": "2025-10-26T12:28:51.548498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Warning: index uniqueness is not enforced, so duplicate indices are possible:\\n\")\n",
    "s_dup = pd.Series([10, 20, 30], index=['a', 'b', 'a'])\n",
    "print(s_dup['a'])  # Returns both values for index 'a'"
   ],
   "id": "37a54aab20536535",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: index uniqueness is not enforced, so duplicate indices are possible:\n",
      "\n",
      "a    10\n",
      "a    30\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T12:48:44.527167Z",
     "start_time": "2025-10-26T12:48:44.523506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Boolean indexing:\\n\")\n",
    "pd.Series([1, 2, 3])[[True, False, True]]"
   ],
   "id": "d91b5942de1ed5a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean indexing:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "2    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have touched  **DatetimeIndex** and **Timestamp**, two examples of a long list of specialized pandas objects for date and time manipulation, see the [documentation](https://pandas.pydata.org/docs/user_guide/timeseries.html) for more details.",
   "id": "2d5a1f1d91dca263"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **3.2.1 Series Operations**\n",
    "\n",
    "Pandas Series support a variety of operations similar to NumPy arrays, including arithmetic operations, aggregation functions, and element-wise operations."
   ],
   "id": "55a5a088455f4a4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T12:03:11.325414Z",
     "start_time": "2025-10-26T12:03:11.319423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "s1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n",
    "s2 = pd.Series([4., 5., 6.], index=['a', 'b', 'c'])\n",
    "\n",
    "print(s1 + s2)   # Addition\n",
    "print(s1 - s2)   # Subtraction\n",
    "print(s1 * s2)   # Multiplication\n",
    "print(s1 / s2)   # Division"
   ],
   "id": "8e9495044ca08058",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    5.0\n",
      "b    7.0\n",
      "c    9.0\n",
      "dtype: float64\n",
      "a   -3.0\n",
      "b   -3.0\n",
      "c   -3.0\n",
      "dtype: float64\n",
      "a     4.0\n",
      "b    10.0\n",
      "c    18.0\n",
      "dtype: float64\n",
      "a    0.25\n",
      "b    0.40\n",
      "c    0.50\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T12:03:39.439797Z",
     "start_time": "2025-10-26T12:03:39.434355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Operations align on index labels:\\n\")\n",
    "s1 = pd.Series([1, 2, 3], index=['a', 'b', 'd'])\n",
    "s2 = pd.Series([4., 5., 6.], index=['a', 'b', 'c'])\n",
    "\n",
    "print(s1 + s2)   # Addition\n",
    "print(s1 - s2)   # Subtraction\n",
    "print(s1 * s2)   # Multiplication\n",
    "print(s1 / s2)   # Division"
   ],
   "id": "de8167ecff8540b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations align on index labels:\n",
      "\n",
      "a    5.0\n",
      "b    7.0\n",
      "c    NaN\n",
      "d    NaN\n",
      "dtype: float64\n",
      "a   -3.0\n",
      "b   -3.0\n",
      "c    NaN\n",
      "d    NaN\n",
      "dtype: float64\n",
      "a     4.0\n",
      "b    10.0\n",
      "c     NaN\n",
      "d     NaN\n",
      "dtype: float64\n",
      "a    0.25\n",
      "b    0.40\n",
      "c     NaN\n",
      "d     NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T12:04:19.305210Z",
     "start_time": "2025-10-26T12:04:19.301863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Comparison operations:\\n\")\n",
    "s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n",
    "print(s > 2)\n",
    "print(s == 2)"
   ],
   "id": "68073dafd0e6c22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    False\n",
      "b    False\n",
      "c     True\n",
      "dtype: bool\n",
      "a    False\n",
      "b     True\n",
      "c    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T12:04:40.538526Z",
     "start_time": "2025-10-26T12:04:40.534469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Aggregation functions:\\n\")\n",
    "s = pd.Series([1, 2, 3, 4, 5])\n",
    "print(s.sum())     # 15\n",
    "print(s.mean())    # 3.0\n",
    "print(s.median())  # 3.0\n",
    "print(s.min())     # 1\n",
    "print(s.max())     # 5\n",
    "print(s.std())     # Standard deviation"
   ],
   "id": "55822079c58e11a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "3.0\n",
      "3.0\n",
      "1\n",
      "5\n",
      "1.5811388300841898\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T12:47:06.511453Z",
     "start_time": "2025-10-26T12:47:06.505591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Element-wise logical operations:\\n\")\n",
    "s1 = pd.Series([True, False, True])\n",
    "s2 = pd.Series([False, True, True])\n",
    "\n",
    "print(s1 & s2)  # Element-wise AND\n",
    "print(s1 | s2)  # Element-wise OR\n",
    "print(~s1)      # NOT"
   ],
   "id": "2068c308c1f73206",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise logical operations:\n",
      "\n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "dtype: bool\n",
      "0    True\n",
      "1    True\n",
      "2    True\n",
      "dtype: bool\n",
      "0    False\n",
      "1     True\n",
      "2    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:45:43.511232Z",
     "start_time": "2025-10-27T11:45:43.479491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"String operations on Series of strings:\\n\")\n",
    "s = pd.Series(['apple', 'banana', 'cherry'])\n",
    "print(s.str.upper())   # ['APPLE', 'BANANA', 'CHERRY']\n",
    "print(s.str.contains('a'))  # [True, True, False]\n",
    "print(s.str.contains('A'))  # case sensitivity\n",
    "print(s.str.len())     # [5, 6, 6]"
   ],
   "id": "19223a5215100ac2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String operations on Series of strings:\n",
      "\n",
      "0     APPLE\n",
      "1    BANANA\n",
      "2    CHERRY\n",
      "dtype: object\n",
      "0     True\n",
      "1     True\n",
      "2    False\n",
      "dtype: bool\n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "dtype: bool\n",
      "0    5\n",
      "1    6\n",
      "2    6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T12:09:35.364575Z",
     "start_time": "2025-10-26T12:09:35.361731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Also NumPy functions work on Series:\\n\")\n",
    "s = pd.Series([1, 4, 9, 16])\n",
    "print(np.sqrt(s))      # Square root\n",
    "print(np.log(s))       # Natural logarithm\n",
    "print(np.exp(s))       # Exponential"
   ],
   "id": "9d719f05c2b60ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Also NumPy functions work on Series:\n",
      "\n",
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "3    4.0\n",
      "dtype: float64\n",
      "0    0.000000\n",
      "1    1.386294\n",
      "2    2.197225\n",
      "3    2.772589\n",
      "dtype: float64\n",
      "0    2.718282e+00\n",
      "1    5.459815e+01\n",
      "2    8.103084e+03\n",
      "3    8.886111e+06\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T12:17:39.543445Z",
     "start_time": "2025-10-26T12:17:39.539990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Cumulative operations:\\n\")\n",
    "s = pd.Series([1, 2, 3, 4, 5])\n",
    "print(s.cumsum())  # Cumulative sum\n",
    "print(s.cumprod()) # Cumulative product\n",
    "print(s.cummax())  # Cumulative maximum\n",
    "print(s.cummin())  # Cumulative minimum"
   ],
   "id": "3644e9dc93704e66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative operations:\n",
      "\n",
      "0     1\n",
      "1     3\n",
      "2     6\n",
      "3    10\n",
      "4    15\n",
      "dtype: int64\n",
      "0      1\n",
      "1      2\n",
      "2      6\n",
      "3     24\n",
      "4    120\n",
      "dtype: int64\n",
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    5\n",
      "dtype: int64\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f557b519916c3abe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:49:52.604412Z",
     "start_time": "2025-10-27T11:49:52.582317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Vectorized operations are efficient:\\n\")\n",
    "s = pd.Series(np.arange(1, 1000001))\n",
    "\n",
    "#squared = s.apply(lambda x: x ** 2)    # loops over each element\n",
    "squared = s ** 2                        # better\n",
    "print(squared)"
   ],
   "id": "edbb1844de7c4928",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized operations are efficient:\n",
      "\n",
      "0                     1\n",
      "1                     4\n",
      "2                     9\n",
      "3                    16\n",
      "4                    25\n",
      "              ...      \n",
      "999995     999992000016\n",
      "999996     999994000009\n",
      "999997     999996000004\n",
      "999998     999998000001\n",
      "999999    1000000000000\n",
      "Length: 1000000, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9e392920b793af8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:57:53.587051Z",
     "start_time": "2025-10-27T11:57:52.995870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_number(x, n):\n",
    "    return x + n\n",
    "\n",
    "# Using apply with a lambda to pass the extra argument\n",
    "added = s.apply(lambda x: add_number(x, 10))    # inefficient\n",
    "# added = add_number(s, 10)                     # good\n",
    "# added = s + 10                                # also good, without newly defined function\n",
    "print(added)"
   ],
   "id": "54d77a4c0f08148b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0              11\n",
      "1              12\n",
      "2              13\n",
      "3              14\n",
      "4              15\n",
      "           ...   \n",
      "999995    1000006\n",
      "999996    1000007\n",
      "999997    1000008\n",
      "999998    1000009\n",
      "999999    1000010\n",
      "Length: 1000000, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **3.3 DataFrames**\n",
    "\n",
    "In data analysis, we often work with tabular data consisting of rows and columns, where similarly to an excel spreadsheet, each row represents a record and each column represents a feature or attribute of that record. DataFrames are the primary data structure in Pandas for handling such tabular data."
   ],
   "id": "98fb5cc6f103de0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T12:08:20.515994Z",
     "start_time": "2025-10-27T12:08:20.479943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 1. Creating DataFrame from a NumPy array\n",
    "# ---------------------------\n",
    "# NumPy array\n",
    "array_data = np.array([[10, 20, 30],\n",
    "                       [40, 50, 60],\n",
    "                       [70, 80, 90]])\n",
    "\n",
    "# Create DataFrame\n",
    "df_from_array = pd.DataFrame(array_data, columns=['A', 'B', 'C'])\n",
    "print(\"DataFrame from NumPy array:\\n\", df_from_array)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Creating DataFrame from a Pandas Series\n",
    "# ---------------------------\n",
    "# Series\n",
    "s1 = pd.Series([100, 200, 300], name='X')\n",
    "s2 = pd.Series([400, 500, 600], name='Y')\n",
    "\n",
    "# Combine Series into DataFrame\n",
    "df_from_series = pd.concat([s1, s2], axis=1) # warning: compare to the case with axis=0\n",
    "print(\"\\nDataFrame from Series:\\n\", df_from_series)\n",
    "# comparison:\n",
    "df_from_series = pd.concat([s1, s2], axis=0)\n",
    "print(\"\\nDataFrame from Series:\\n\", df_from_series)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Creating DataFrame from a dictionary\n",
    "# ---------------------------\n",
    "# Dictionary\n",
    "dict_data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 22],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago']\n",
    "}\n",
    "\n",
    "df_from_dict = pd.DataFrame(dict_data)\n",
    "print(\"\\nDataFrame from Dictionary:\\n\", df_from_dict)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Creating DataFrame from a csv file\n",
    "# ---------------------------\n",
    "\n",
    "df_from_csv = pd.read_csv('data/csv_example.csv') # Assuming a CSV file 'csv_example.csv' exists in the current 'data' directory\n",
    "print(\"\\nDataFrame from CSV file:\\n\", df_from_csv, sep=',') # Not convinced about 'separator' importance..."
   ],
   "id": "c2e8cff4cd412a51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame from NumPy array:\n",
      "     A   B   C\n",
      "0  10  20  30\n",
      "1  40  50  60\n",
      "2  70  80  90\n",
      "\n",
      "DataFrame from Series:\n",
      "      X    Y\n",
      "0  100  400\n",
      "1  200  500\n",
      "2  300  600\n",
      "\n",
      "DataFrame from Series:\n",
      " 0    100\n",
      "1    200\n",
      "2    300\n",
      "0    400\n",
      "1    500\n",
      "2    600\n",
      "dtype: int64\n",
      "\n",
      "DataFrame from Dictionary:\n",
      "       Name  Age         City\n",
      "0    Alice   25     New York\n",
      "1      Bob   30  Los Angeles\n",
      "2  Charlie   22      Chicago\n",
      "\n",
      "DataFrame from CSV file:\n",
      "    EmployeeID             Name Department  Age  Salary JoiningDate FullTime\n",
      "0         101       John Smith      Sales   28   50000  2019-03-15      Yes\n",
      "1         102         Jane Doe  Marketing   34   60000  2018-07-23      Yes\n",
      "2         103     Mike Johnson         IT   30   55000  2020-01-10       No\n",
      "3         104      Emily Davis         HR   26   48000  2021-06-12      Yes\n",
      "4         105    William Brown         IT   40   75000  2017-11-03      Yes\n",
      "5         106     Linda Wilson      Sales   32   52000  2019-08-19       No\n",
      "6         107        David Lee  Marketing   29   58000  2020-02-25      Yes\n",
      "7         108        Sarah Kim         HR   35   62000  2016-05-30      Yes\n",
      "8         109      James White         IT   31   54000  2019-09-01       No\n",
      "9         110  Patricia Taylor      Sales   27   50000  2021-01-20      Yes\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T13:00:43.230393Z",
     "start_time": "2025-10-26T13:00:43.226325Z"
    }
   },
   "cell_type": "code",
   "source": "df_from_csv.shape, df_from_csv.columns, df_from_csv.index, df_from_csv.dtypes",
   "id": "7c6c4fde632a841d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 7),\n",
       " Index(['EmployeeID', 'Name', 'Department', 'Age', 'Salary', 'JoiningDate',\n",
       "        'FullTime'],\n",
       "       dtype='object'),\n",
       " RangeIndex(start=0, stop=10, step=1),\n",
       " EmployeeID      int64\n",
       " Name           object\n",
       " Department     object\n",
       " Age             int64\n",
       " Salary          int64\n",
       " JoiningDate    object\n",
       " FullTime       object\n",
       " dtype: object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T13:09:07.516165Z",
     "start_time": "2025-10-26T13:09:07.512956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_from_csv = df_from_csv.astype({'Salary': float})\n",
    "df_from_csv.dtypes"
   ],
   "id": "29075925a7533551",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmployeeID       int64\n",
       "Name            object\n",
       "Department      object\n",
       "Age              int64\n",
       "Salary         float64\n",
       "JoiningDate     object\n",
       "FullTime        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T12:13:30.140780Z",
     "start_time": "2025-10-27T12:13:30.089219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "    'Age': [24, 27, 22, 32, 29],\n",
    "    'Major': ['Math', 'Physics', 'Biology', 'CS', 'Chemistry'],\n",
    "    'GPA': [3.5, 3.8, 3.2, 3.9, 3.7]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Setting an index\n",
    "# ---------------------------\n",
    "df.set_index('Name', inplace=True)\n",
    "print(\"\\nDataFrame with 'Name' as index:\\n\", df)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Slicing rows\n",
    "# ---------------------------\n",
    "# Using loc (label-based)\n",
    "print(\"\\nSlice using loc (Alice to Charlie):\\n\", df.loc['Alice':'Charlie'])\n",
    "\n",
    "# Using iloc (position-based)\n",
    "print(\"\\nSlice using iloc (first 3 rows):\\n\", df.iloc[0:3])\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Slicing columns\n",
    "# ---------------------------\n",
    "# Select a single column\n",
    "print(\"\\nSelect 'Age' column:\\n\", df['Age'])\n",
    "\n",
    "# Select multiple columns\n",
    "print(\"\\nSelect 'Age' and 'GPA' columns:\\n\", df[['Age', 'GPA']])\n",
    "\n",
    "# Select columns using loc (rows + columns)\n",
    "print(\"\\nSelect rows Alice to Charlie and columns Age & GPA:\\n\", df.loc['Alice':'Charlie', ['Age', 'GPA']])\n",
    "\n",
    "# Select columns using iloc (by positions)\n",
    "# Rows 0 to 2, columns 0 to 1 (Age and Major)\n",
    "print(\"\\nSelect first 3 rows and first 2 columns using iloc:\\n\", df.iloc[0:3, 0:2])\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Filtering rows\n",
    "# ---------------------------\n",
    "# Filter students with GPA > 3.5\n",
    "high_gpa = df[df['GPA'] > 3.5]\n",
    "print(\"\\nStudents with GPA > 3.5:\\n\", high_gpa)\n",
    "\n",
    "# Filter students majoring in CS or Physics\n",
    "selected_majors = df[df['Major'].isin(['CS', 'Physics'])]\n",
    "print(\"\\nStudents majoring in CS or Physics:\\n\", selected_majors)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Changing values\n",
    "# ---------------------------\n",
    "# Change a single value using loc\n",
    "df.loc['Alice', 'GPA'] = 3.6\n",
    "print(\"\\nAfter changing Alice's GPA to 3.6:\\n\", df)\n",
    "\n",
    "# Change multiple values using condition\n",
    "df.loc[df['Major'] == 'CS', 'GPA'] = 4.0\n",
    "print(\"\\nAfter setting GPA=4.0 for CS majors:\\n\", df)\n",
    "\n",
    "# Change an entire column\n",
    "df['Age'] = df['Age'] + 1  # increment age by 1\n",
    "print(\"\\nAfter incrementing Age by 1:\\n\", df)"
   ],
   "id": "d49a2ac9af024ded",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "       Name  Age      Major  GPA\n",
      "0    Alice   24       Math  3.5\n",
      "1      Bob   27    Physics  3.8\n",
      "2  Charlie   22    Biology  3.2\n",
      "3    David   32         CS  3.9\n",
      "4      Eva   29  Chemistry  3.7\n",
      "\n",
      "DataFrame with 'Name' as index:\n",
      "          Age      Major  GPA\n",
      "Name                        \n",
      "Alice     24       Math  3.5\n",
      "Bob       27    Physics  3.8\n",
      "Charlie   22    Biology  3.2\n",
      "David     32         CS  3.9\n",
      "Eva       29  Chemistry  3.7\n",
      "\n",
      "Slice using loc (Alice to Charlie):\n",
      "          Age    Major  GPA\n",
      "Name                      \n",
      "Alice     24     Math  3.5\n",
      "Bob       27  Physics  3.8\n",
      "Charlie   22  Biology  3.2\n",
      "\n",
      "Slice using iloc (first 3 rows):\n",
      "          Age    Major  GPA\n",
      "Name                      \n",
      "Alice     24     Math  3.5\n",
      "Bob       27  Physics  3.8\n",
      "Charlie   22  Biology  3.2\n",
      "\n",
      "Select 'Age' column:\n",
      " Name\n",
      "Alice      24\n",
      "Bob        27\n",
      "Charlie    22\n",
      "David      32\n",
      "Eva        29\n",
      "Name: Age, dtype: int64\n",
      "\n",
      "Select 'Age' and 'GPA' columns:\n",
      "          Age  GPA\n",
      "Name             \n",
      "Alice     24  3.5\n",
      "Bob       27  3.8\n",
      "Charlie   22  3.2\n",
      "David     32  3.9\n",
      "Eva       29  3.7\n",
      "\n",
      "Select rows Alice to Charlie and columns Age & GPA:\n",
      "          Age  GPA\n",
      "Name             \n",
      "Alice     24  3.5\n",
      "Bob       27  3.8\n",
      "Charlie   22  3.2\n",
      "\n",
      "Select first 3 rows and first 2 columns using iloc:\n",
      "          Age    Major\n",
      "Name                 \n",
      "Alice     24     Math\n",
      "Bob       27  Physics\n",
      "Charlie   22  Biology\n",
      "\n",
      "Students with GPA > 3.5:\n",
      "        Age      Major  GPA\n",
      "Name                      \n",
      "Bob     27    Physics  3.8\n",
      "David   32         CS  3.9\n",
      "Eva     29  Chemistry  3.7\n",
      "\n",
      "Students majoring in CS or Physics:\n",
      "        Age    Major  GPA\n",
      "Name                    \n",
      "Bob     27  Physics  3.8\n",
      "David   32       CS  3.9\n",
      "\n",
      "After changing Alice's GPA to 3.6:\n",
      "          Age      Major  GPA\n",
      "Name                        \n",
      "Alice     24       Math  3.6\n",
      "Bob       27    Physics  3.8\n",
      "Charlie   22    Biology  3.2\n",
      "David     32         CS  3.9\n",
      "Eva       29  Chemistry  3.7\n",
      "\n",
      "After setting GPA=4.0 for CS majors:\n",
      "          Age      Major  GPA\n",
      "Name                        \n",
      "Alice     24       Math  3.6\n",
      "Bob       27    Physics  3.8\n",
      "Charlie   22    Biology  3.2\n",
      "David     32         CS  4.0\n",
      "Eva       29  Chemistry  3.7\n",
      "\n",
      "After incrementing Age by 1:\n",
      "          Age      Major  GPA\n",
      "Name                        \n",
      "Alice     25       Math  3.6\n",
      "Bob       28    Physics  3.8\n",
      "Charlie   23    Biology  3.2\n",
      "David     33         CS  4.0\n",
      "Eva       30  Chemistry  3.7\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T12:19:54.782087Z",
     "start_time": "2025-10-27T12:19:54.764244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"When updating values using different dataframe, we must be careful.\")\n",
    "df1 = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [10, 20, 30, 40, 50],\n",
    "    'C': [100, 200, 300, 400, 500]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'A': [9, 8, 7, 6, 5],\n",
    "    'B': [90, 80, 70, 60, 50],\n",
    "    'C': [900, 800, 700, 600, 500]\n",
    "})\n",
    "\n",
    "# Without .values\n",
    "df1.loc[1:2, ['B', 'C']] = df2.loc[3:4, ['B', 'C']]  # it searches for index match in this case\n",
    "print(df1)"
   ],
   "id": "82457314de92b92d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When updating values using different dataframe, we must be careful.\n",
      "    B    C\n",
      "3  60  600\n",
      "4  50  500\n",
      "     A     B      C\n",
      "0  1.0  10.0  100.0\n",
      "1  2.0   NaN    NaN\n",
      "2  3.0   NaN    NaN\n",
      "3  4.0  40.0  400.0\n",
      "4  5.0  50.0  500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salav\\AppData\\Local\\Temp\\ipykernel_42936\\1001147925.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[[nan nan]\n",
      " [nan nan]]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df1.loc[1:2, ['B', 'C']] = df2.loc[3:4, ['B', 'C']]  # it searches for index match in this case\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T07:20:44.901017Z",
     "start_time": "2025-10-27T07:20:44.897434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df1.loc[1:2, ['B', 'C']] = df2.loc[1:2, ['B', 'C']]\n",
    "print(df1)"
   ],
   "id": "b977ae3c19c7e202",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B      C\n",
      "0  1.0  10.0  100.0\n",
      "1  2.0  80.0  800.0\n",
      "2  3.0  70.0  700.0\n",
      "3  4.0  40.0  400.0\n",
      "4  5.0  50.0  500.0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T07:20:45.424261Z",
     "start_time": "2025-10-27T07:20:45.421184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df1.loc[1:2, ['B', 'C']] = df2.loc[3:4, ['B', 'C']].values\n",
    "print(df1)"
   ],
   "id": "37e639199a84b33f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B      C\n",
      "0  1.0  10.0  100.0\n",
      "1  2.0  60.0  600.0\n",
      "2  3.0  50.0  500.0\n",
      "3  4.0  40.0  400.0\n",
      "4  5.0  50.0  500.0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another major topic is grouping and aggregation in DataFrames, which allows us to perform operations on subsets of data based on certain criteria. This topic will be covered in more detail in later assignments.",
   "id": "7f08638c37f3a0e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **3.3.1 Inplace vs non-inplace**",
   "id": "91cc99fc7842d891"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T13:15:45.125150Z",
     "start_time": "2025-10-26T13:15:45.121015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = {\n",
    "    'Name': ['Alice', 'Bob', None, 'David'],\n",
    "    'Age': [25, np.nan, 30, np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ],
   "id": "b87b00c09e6944b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name   Age\n",
      "0  Alice  25.0\n",
      "1    Bob   NaN\n",
      "2   None  30.0\n",
      "3  David   NaN\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T13:16:01.550459Z",
     "start_time": "2025-10-26T13:16:01.544664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_df = df.dropna(subset=['Name'])\n",
    "print(new_df)  # New df without NaN\n",
    "print(df)      # Original df unchanged"
   ],
   "id": "128a7c73e6bdf44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name   Age\n",
      "0  Alice  25.0\n",
      "1    Bob   NaN\n",
      "3  David   NaN\n",
      "    Name   Age\n",
      "0  Alice  25.0\n",
      "1    Bob   NaN\n",
      "2   None  30.0\n",
      "3  David   NaN\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T13:16:20.946673Z",
     "start_time": "2025-10-26T13:16:20.943076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.dropna(subset=['Name'], inplace=True)\n",
    "print(df)"
   ],
   "id": "dc1cea54ae3427e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name   Age\n",
      "0  Alice  25.0\n",
      "1    Bob   NaN\n",
      "3  David   NaN\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T13:18:16.259207Z",
     "start_time": "2025-10-26T13:18:16.247600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Warning: inplace=True modifies the original DataFrame and returns None, chaining the operations is therefore impossible.\\n\")\n",
    "type(df.dropna(subset=['Name'], inplace=True).fillna(0))"
   ],
   "id": "9bc84a6e29e20687",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inplace=True modifies the original DataFrame and returns None, chaining the operations is therefore impossible.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'fillna'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[96]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mWarning: inplace=True modifies the original DataFrame and returns None, chaining the operations is therefore impossible.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28mtype\u001B[39m(\u001B[43mdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdropna\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubset\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mName\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfillna\u001B[49m(\u001B[32m0\u001B[39m))\n",
      "\u001B[31mAttributeError\u001B[39m: 'NoneType' object has no attribute 'fillna'"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **3.3.2 Transformations**",
   "id": "2db3685f4072f184"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T13:25:23.642625Z",
     "start_time": "2025-10-26T13:25:23.637553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Product': ['A', 'B', 'C', 'D'],\n",
    "    'Price': [100, 200, 150, 300],\n",
    "    'Quantity': [5, 2, 7, 3]\n",
    "})\n",
    "\n",
    "# 1. Adding new columns\n",
    "df['Total'] = df['Price'] * df['Quantity']\n",
    "\n",
    "# 2. apply(), map()\n",
    "df['Discounted'] = df['Price'].apply(lambda x: x*0.9)  # Apply to column\n",
    "df['Category'] = df['Product'].map({'A':'X','B':'Y','C':'X','D':'Y'})  # Map values\n",
    "\n",
    "# 3. replace()\n",
    "df['Category'] = df['Category'].replace({'X':'Type1','Y':'Type2'})\n",
    "\n",
    "# 4. Binning data\n",
    "df['PriceRange'] = pd.cut(df['Price'], bins=[0, 100, 200, 500], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# 5. Categorical handling\n",
    "df['Category'] = df['Category'].astype('category')"
   ],
   "id": "5768fea3e9a895fa",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **3.3.3 Date and Time in DataFrames**",
   "id": "74fe096115c244c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T12:28:37.888138Z",
     "start_time": "2025-10-27T12:28:37.749530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = {\n",
    "    'event': ['Event A', 'Event B', 'Event C', 'Event D'],\n",
    "    'date_str': ['2025-10-01', '2025-10-05', '2025-11-10', '2025-12-15'],\n",
    "    'time_str': ['12:30:00', '15:45:00', '09:20:00', '18:00:00']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Converting strings to datetime\n",
    "# ------------------------------\n",
    "df['date'] = pd.to_datetime(df['date_str'])\n",
    "df['datetime'] = pd.to_datetime(df['date_str'] + ' ' + df['time_str'])\n",
    "print(\"\\nDataFrame with datetime:\\n\", df)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Extracting components\n",
    "# ------------------------------\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['weekday'] = df['date'].dt.day_name()\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['minute'] = df['datetime'].dt.minute\n",
    "df['second'] = df['datetime'].dt.second\n",
    "print(\"\\nExtracted date components:\\n\", df)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Date arithmetic\n",
    "# ------------------------------\n",
    "df['next_day'] = df['date'] + pd.Timedelta(days=1)\n",
    "df['prev_week'] = df['date'] - pd.Timedelta(weeks=1)\n",
    "df['plus_hours'] = df['datetime'] + pd.Timedelta(hours=5)\n",
    "print(\"\\nDate arithmetic:\\n\", df)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Filtering by date\n",
    "# ------------------------------\n",
    "# Filter events after 2025-11-01\n",
    "filtered_df = df[df['date'] > '2025-11-01']\n",
    "print(\"\\nFiltered events after 2025-11-01:\\n\", filtered_df)\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Date ranges\n",
    "# ------------------------------\n",
    "date_range = pd.date_range(start='2025-10-01', end='2025-10-10', freq='D')\n",
    "print(\"\\nDate range from 2025-10-01 to 2025-10-10:\\n\", date_range)\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Resampling and time series\n",
    "# ------------------------------\n",
    "# Creating a time series\n",
    "ts_data = pd.Series(np.random.randn(10), \n",
    "                    index=pd.date_range('2025-10-01', periods=10, freq='D'))\n",
    "print(\"\\nTime series data:\\n\", ts_data)\n",
    "\n",
    "# Resample to weekly sums\n",
    "weekly_sum = ts_data.resample('W').sum()\n",
    "print(\"\\nWeekly sum:\\n\", weekly_sum)\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Handling missing dates\n",
    "# ------------------------------\n",
    "ts_data_with_missing = ts_data.copy()\n",
    "ts_data_with_missing = ts_data_with_missing.drop(ts_data_with_missing.index[3])\n",
    "print(\"\\nTime series with missing date:\\n\", ts_data_with_missing)\n",
    "\n",
    "# Filling missing dates\n",
    "ts_filled = ts_data_with_missing.asfreq('D', fill_value=0)\n",
    "print(\"\\nFilled missing dates:\\n\", ts_filled)\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Date formatting\n",
    "# ------------------------------\n",
    "df['formatted_date'] = df['date'].dt.strftime('%d-%b-%Y')\n",
    "df['formatted_datetime'] = df['datetime'].dt.strftime('%Y/%m/%d %H:%M:%S')\n",
    "print(\"\\nFormatted dates:\\n\", df[['formatted_date', 'formatted_datetime']])\n",
    "\n",
    "# ------------------------------\n",
    "# 10. Working with periods\n",
    "# ------------------------------\n",
    "period = pd.Period('2025-10', freq='M')  # Month period\n",
    "print(\"\\nMonthly period:\", period)\n",
    "print(\"Start of period:\", period.start_time)\n",
    "print(\"End of period:\", period.end_time)"
   ],
   "id": "7d0f9b516093415f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      event    date_str  time_str\n",
      "0  Event A  2025-10-01  12:30:00\n",
      "1  Event B  2025-10-05  15:45:00\n",
      "2  Event C  2025-11-10  09:20:00\n",
      "3  Event D  2025-12-15  18:00:00\n",
      "\n",
      "DataFrame with datetime:\n",
      "      event    date_str  time_str       date            datetime\n",
      "0  Event A  2025-10-01  12:30:00 2025-10-01 2025-10-01 12:30:00\n",
      "1  Event B  2025-10-05  15:45:00 2025-10-05 2025-10-05 15:45:00\n",
      "2  Event C  2025-11-10  09:20:00 2025-11-10 2025-11-10 09:20:00\n",
      "3  Event D  2025-12-15  18:00:00 2025-12-15 2025-12-15 18:00:00\n",
      "\n",
      "Extracted date components:\n",
      "      event    date_str  time_str       date            datetime  year  month  \\\n",
      "0  Event A  2025-10-01  12:30:00 2025-10-01 2025-10-01 12:30:00  2025     10   \n",
      "1  Event B  2025-10-05  15:45:00 2025-10-05 2025-10-05 15:45:00  2025     10   \n",
      "2  Event C  2025-11-10  09:20:00 2025-11-10 2025-11-10 09:20:00  2025     11   \n",
      "3  Event D  2025-12-15  18:00:00 2025-12-15 2025-12-15 18:00:00  2025     12   \n",
      "\n",
      "   day    weekday  hour  minute  second  \n",
      "0    1  Wednesday    12      30       0  \n",
      "1    5     Sunday    15      45       0  \n",
      "2   10     Monday     9      20       0  \n",
      "3   15     Monday    18       0       0  \n",
      "\n",
      "Date arithmetic:\n",
      "      event    date_str  time_str       date            datetime  year  month  \\\n",
      "0  Event A  2025-10-01  12:30:00 2025-10-01 2025-10-01 12:30:00  2025     10   \n",
      "1  Event B  2025-10-05  15:45:00 2025-10-05 2025-10-05 15:45:00  2025     10   \n",
      "2  Event C  2025-11-10  09:20:00 2025-11-10 2025-11-10 09:20:00  2025     11   \n",
      "3  Event D  2025-12-15  18:00:00 2025-12-15 2025-12-15 18:00:00  2025     12   \n",
      "\n",
      "   day    weekday  hour  minute  second   next_day  prev_week  \\\n",
      "0    1  Wednesday    12      30       0 2025-10-02 2025-09-24   \n",
      "1    5     Sunday    15      45       0 2025-10-06 2025-09-28   \n",
      "2   10     Monday     9      20       0 2025-11-11 2025-11-03   \n",
      "3   15     Monday    18       0       0 2025-12-16 2025-12-08   \n",
      "\n",
      "           plus_hours  \n",
      "0 2025-10-01 17:30:00  \n",
      "1 2025-10-05 20:45:00  \n",
      "2 2025-11-10 14:20:00  \n",
      "3 2025-12-15 23:00:00  \n",
      "\n",
      "Filtered events after 2025-11-01:\n",
      "      event    date_str  time_str       date            datetime  year  month  \\\n",
      "2  Event C  2025-11-10  09:20:00 2025-11-10 2025-11-10 09:20:00  2025     11   \n",
      "3  Event D  2025-12-15  18:00:00 2025-12-15 2025-12-15 18:00:00  2025     12   \n",
      "\n",
      "   day weekday  hour  minute  second   next_day  prev_week          plus_hours  \n",
      "2   10  Monday     9      20       0 2025-11-11 2025-11-03 2025-11-10 14:20:00  \n",
      "3   15  Monday    18       0       0 2025-12-16 2025-12-08 2025-12-15 23:00:00  \n",
      "\n",
      "Date range from 2025-10-01 to 2025-10-10:\n",
      " DatetimeIndex(['2025-10-01', '2025-10-02', '2025-10-03', '2025-10-04',\n",
      "               '2025-10-05', '2025-10-06', '2025-10-07', '2025-10-08',\n",
      "               '2025-10-09', '2025-10-10'],\n",
      "              dtype='datetime64[ns]', freq='D')\n",
      "\n",
      "Time series data:\n",
      " 2025-10-01    0.492393\n",
      "2025-10-02   -1.265197\n",
      "2025-10-03   -0.342443\n",
      "2025-10-04   -1.286780\n",
      "2025-10-05   -0.676869\n",
      "2025-10-06   -1.353263\n",
      "2025-10-07   -1.981768\n",
      "2025-10-08   -0.169392\n",
      "2025-10-09   -0.741056\n",
      "2025-10-10   -0.669353\n",
      "Freq: D, dtype: float64\n",
      "\n",
      "Weekly sum:\n",
      " 2025-10-05   -3.078895\n",
      "2025-10-12   -4.914832\n",
      "Freq: W-SUN, dtype: float64\n",
      "\n",
      "Time series with missing date:\n",
      " 2025-10-01    0.492393\n",
      "2025-10-02   -1.265197\n",
      "2025-10-03   -0.342443\n",
      "2025-10-05   -0.676869\n",
      "2025-10-06   -1.353263\n",
      "2025-10-07   -1.981768\n",
      "2025-10-08   -0.169392\n",
      "2025-10-09   -0.741056\n",
      "2025-10-10   -0.669353\n",
      "dtype: float64\n",
      "\n",
      "Filled missing dates:\n",
      " 2025-10-01    0.492393\n",
      "2025-10-02   -1.265197\n",
      "2025-10-03   -0.342443\n",
      "2025-10-04    0.000000\n",
      "2025-10-05   -0.676869\n",
      "2025-10-06   -1.353263\n",
      "2025-10-07   -1.981768\n",
      "2025-10-08   -0.169392\n",
      "2025-10-09   -0.741056\n",
      "2025-10-10   -0.669353\n",
      "Freq: D, dtype: float64\n",
      "\n",
      "Formatted dates:\n",
      "   formatted_date   formatted_datetime\n",
      "0    01-Oct-2025  2025/10/01 12:30:00\n",
      "1    05-Oct-2025  2025/10/05 15:45:00\n",
      "2    10-Nov-2025  2025/11/10 09:20:00\n",
      "3    15-Dec-2025  2025/12/15 18:00:00\n",
      "\n",
      "Monthly period: 2025-10\n",
      "Start of period: 2025-10-01 00:00:00\n",
      "End of period: 2025-10-31 23:59:59.999999999\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.4 Homework: Working with Pandas DataFrames in Data Science\n",
    "\n",
    "### Task overview\n",
    "In this assignment, you will practice working with **Pandas DataFrames** to perform essential data analysis and preprocessing tasks. You will simulate a small part of a **sensor data analytics pipeline**, a common workflow in data science used to clean, transform, and analyze time-series measurements collected from multiple sensors.  \n",
    "\n",
    "### Your Task\n",
    "You are given a dataset stored in **`data/sensor_data.csv`**, containing measurements from multiple sensors, including temperature and humidity readings at various timestamps.\n",
    "\n",
    "### 1. **Load the Data**\n",
    "- Load the CSV file into a Pandas DataFrame named `df`.\n",
    "- Convert columns to appropriate data types (e.g., numeric, datetime, categorical).\n",
    "- Set a **MultiIndex** using the combination of `sensor_id` and `timestamp`.  \n",
    "  *Hint:* See [Pandas MultiIndex documentation](https://pandas.pydata.org/docs/user_guide/advanced.html).\n",
    "\n",
    "### 2. **Data Cleaning**\n",
    "- Remove all rows where:\n",
    "  - `temperature` is **below the 20th percentile** or **above the 80th percentile**, **and**\n",
    "  - any column has a missing value.  \n",
    "  These rows should be treated as *outliers*.\n",
    "- Report **how many rows were removed**.\n",
    "\n",
    "\n",
    "### 3. **Filtering and Feature Engineering**\n",
    "Perform the following operations on the cleaned DataFrame:\n",
    "\n",
    "1. Count how many observations have:\n",
    "   - `temperature` > 25, **and**\n",
    "   - `humidity` < 40.\n",
    "2. Create a new column that classifies each observation based on `temperature`:\n",
    "   - `\"low\"` if below 20¬∞C  \n",
    "   - `\"medium\"` if between 20¬∞C and 30¬∞C  \n",
    "   - `\"high\"` if above 30¬∞C\n",
    "3. Drop all rows where:\n",
    "   - `sensor_id` == `\"S5\"`, **and**\n",
    "   - `temperature` lies **outside one standard deviation** from the **global (overall)** mean temperature.\n",
    "\n",
    "### 4. **Time and Date Analysis**\n",
    "Add the following time-based columns:\n",
    "\n",
    "1. **`hours_from_start`** ‚Äì the difference in hours between each `timestamp` and the first timestamp in the dataset.  \n",
    "2. **`is_daytime`** ‚Äì `True` if the measurement was taken between **6:00 AM and 6:00 PM**, otherwise `False`.  \n",
    "3. **`after_12h`** ‚Äì `True` if the measurement occurred **at least 12 hours after the first measurement** (you may optionally compute this per sensor).  \n",
    "4. For **`sensor_id == \"S2\"`**, perform the following:\n",
    "   - Create a **rolling average** of `temperature` over a **30-minute window**.\n",
    "   - Compute the **difference** between the current and next `humidity` reading.\n",
    "   - Create a Boolean column indicating whether:\n",
    "     - `temperature` has increased by **more than 5¬∞C**, and  \n",
    "     - `humidity` has dropped by **more than 20** compared to the previous measurement.  \n",
    "   - Save this subset of data to **`data/sensor_data_S2.csv`**.\n",
    "5. **`is_weekend`** ‚Äì indicate whether the measurement was taken on a **Saturday or Sunday**.\n",
    "\n",
    "### Hints\n",
    "- When computing statistics along columns, use `axis=0`.\n",
    "- Prefer **vectorized operations** over `for` loops for efficiency.\n",
    "- Add **comments or docstrings** to explain key parts of your code.\n"
   ],
   "id": "d6026316728d0074"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:55:38.069888Z",
     "start_time": "2025-11-01T13:55:37.946654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TASK 1 - Load the Data\n",
    "df = pd.read_csv('data/sensor_data.csv')\n",
    "df['sensor_id'] = df['sensor_id'].astype('category')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "multi = pd.MultiIndex.from_frame(df[['sensor_id', 'timestamp']])\n",
    "df = df.set_index(multi)\n",
    "\n",
    "# TASK 2 - Data Cleaning\n",
    "number_of_rows_old = df.shape[0]\n",
    "lower_quantile = np.percentile(df['temperature'], 20)\n",
    "upper_quantile = np.percentile(df['temperature'], 80)\n",
    "#print(\"Number of rows containing NaN:\", df[df.isna().any(axis=1)].shape[0])\n",
    "df = df[(df['temperature'] >= lower_quantile) & (df['temperature'] <= upper_quantile)]\n",
    "df = df.dropna()\n",
    "print(\"Number of rows removed (outliers): \", number_of_rows_old - df.shape[0])\n",
    "\n",
    "# TASK 3 - Filtering and Feature Engineering\n",
    "print(\"Number of observations with temperature > 25, and humidity < 40: \",\n",
    "      df[(df['temperature'] > 25) & (df['humidity'] < 40)].shape[0])\n",
    " # all observations have temp in [lower_quantile, upper_quantile] = [22.33, 23.46]\n",
    " # no sense in making new column for temperature labels <20¬∞C, 20-30¬∞C, >30¬∞C\n",
    "df['temperature_bins'] = pd.cut(df['temperature'], bins=[0, 20, 30, 50], labels=['Low', 'Medium', 'High'])\n",
    "mean_temp = df['temperature'].mean()\n",
    "std_temp = df['temperature'].std()\n",
    "\n",
    "df = df[~((df['sensor_id'] == \"S5\") &\n",
    "         ((df['temperature'] > mean_temp + std_temp) | (df['temperature'] < mean_temp - std_temp)))]\n",
    " # from assignment ain't clear if it should be as above, or:\n",
    " # df = df[~(df['sensor_id'] == \"S5\")]\n",
    " # df = df[~((df['temperature'] > mean_temp + std_temp) | (df['temperature'] < mean_temp - std_temp))]\n",
    "\n",
    "# TASK 4 - Time and Date Analysis\n",
    "first_timestamp = df['timestamp'].min()\n",
    "df['hours_from_start'] = (df['timestamp'] - first_timestamp).dt.total_seconds() / 3600\n",
    "df['is_daytime'] = ((df['timestamp'].dt.hour > 6) & (df['timestamp'].dt.hour < 18))\n",
    "df['after_12h'] =  df['timestamp'] > first_timestamp + pd.Timedelta(hours=12)\n",
    "\n",
    "sensor2 = df.loc['S2'].copy()\n",
    "sensor2['rolling_average_temp'] = sensor2['temperature'].rolling('30min').mean()\n",
    " # for timestamp XXX is rolling_average_temp mean over temp from time interval (XXX-30min, XXX]\n",
    "sensor2['humidity_diff'] = sensor2['humidity'] - sensor2['humidity'].shift(-1)\n",
    " # might be better difference between next and current humidity reading... for better interpretation\n",
    "sensor2['big_temp_increase'] = (sensor2['temperature'] - sensor2['temperature'].shift(1)) > 5\n",
    "sensor2['big_humid_decrease'] = (sensor2['humidity'] - sensor2['humidity'].shift(1)) < -20\n",
    " # max difference of temperatures overall ain't bigger than 1¬∞C\n",
    " # all humidity data in (42, 56)\n",
    "sensor2.to_csv('data/sensor_data_S2.csv')\n",
    "\n",
    "df['is_weekend'] = df['timestamp'].dt.dayofweek >= 5"
   ],
   "id": "bbe094978b29ce1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows removed (outliers):  495\n",
      "Number of observations with temperature > 25, and humidity < 40:  0\n",
      "                                        timestamp sensor_id  temperature  \\\n",
      "sensor_id timestamp                                                        \n",
      "S1        2025-10-30 08:00:00 2025-10-30 08:00:00        S1        22.51   \n",
      "          2025-10-30 08:10:00 2025-10-30 08:10:00        S1        22.63   \n",
      "          2025-10-30 08:20:00 2025-10-30 08:20:00        S1        22.66   \n",
      "          2025-10-30 08:45:00 2025-10-30 08:45:00        S1        22.36   \n",
      "          2025-10-30 09:00:00 2025-10-30 09:00:00        S1        22.94   \n",
      "...                                           ...       ...          ...   \n",
      "S5        2025-10-31 03:35:00 2025-10-31 03:35:00        S5        22.75   \n",
      "          2025-10-31 03:40:00 2025-10-31 03:40:00        S5        23.19   \n",
      "          2025-10-31 03:45:00 2025-10-31 03:45:00        S5        23.21   \n",
      "          2025-10-31 04:10:00 2025-10-31 04:10:00        S5        22.98   \n",
      "          2025-10-31 04:35:00 2025-10-31 04:35:00        S5        23.22   \n",
      "\n",
      "                               humidity temperature_bins  hours_from_start  \\\n",
      "sensor_id timestamp                                                          \n",
      "S1        2025-10-30 08:00:00     46.67           Medium          0.000000   \n",
      "          2025-10-30 08:10:00     43.77           Medium          0.166667   \n",
      "          2025-10-30 08:20:00     47.45           Medium          0.333333   \n",
      "          2025-10-30 08:45:00     45.02           Medium          0.750000   \n",
      "          2025-10-30 09:00:00     47.02           Medium          1.000000   \n",
      "...                                 ...              ...               ...   \n",
      "S5        2025-10-31 03:35:00     50.25           Medium         19.583333   \n",
      "          2025-10-31 03:40:00     47.84           Medium         19.666667   \n",
      "          2025-10-31 03:45:00     50.13           Medium         19.750000   \n",
      "          2025-10-31 04:10:00     49.89           Medium         20.166667   \n",
      "          2025-10-31 04:35:00     48.78           Medium         20.583333   \n",
      "\n",
      "                               is_daytime  after_12h  is_weekend  \n",
      "sensor_id timestamp                                               \n",
      "S1        2025-10-30 08:00:00        True      False       False  \n",
      "          2025-10-30 08:10:00        True      False       False  \n",
      "          2025-10-30 08:20:00        True      False       False  \n",
      "          2025-10-30 08:45:00        True      False       False  \n",
      "          2025-10-30 09:00:00        True      False       False  \n",
      "...                                   ...        ...         ...  \n",
      "S5        2025-10-31 03:35:00       False       True       False  \n",
      "          2025-10-31 03:40:00       False       True       False  \n",
      "          2025-10-31 03:45:00       False       True       False  \n",
      "          2025-10-31 04:10:00       False       True       False  \n",
      "          2025-10-31 04:35:00       False       True       False  \n",
      "\n",
      "[692 rows x 9 columns]\n"
     ]
    }
   ],
   "execution_count": 163
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
